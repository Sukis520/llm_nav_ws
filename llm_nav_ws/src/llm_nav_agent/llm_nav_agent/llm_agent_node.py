#!/usr/bin/env python3
"""
LLM Navigation Agent - ROS2 ä¸»èŠ‚ç‚¹
ç»“åˆ LangChain + Nav2 å®ç°è‡ªç„¶è¯­è¨€æ§åˆ¶æœºå™¨äººå¯¼èˆª
"""

import os
import threading
from typing import List, Optional

import rclpy
from rclpy.node import Node
from rclpy.callback_groups import ReentrantCallbackGroup
from rclpy.executors import MultiThreadedExecutor
from std_msgs.msg import String

# LangChain ç›¸å…³
from langchain_openai import ChatOpenAI
from langchain_community.chat_models import ChatOllama
from langchain.agents import AgentExecutor, create_tool_calling_agent
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage, BaseMessage

# æœ¬åœ°æ¨¡å—
from .nav_tools import ALL_NAV_TOOLS, Nav2ToolKit
from .prompts import get_prompt_manager, PromptManager
from .utils import LocationManager, get_default_location_manager


class LLMNavAgentNode(Node):
    """
    LLM å¯¼èˆªä»£ç† ROS2 èŠ‚ç‚¹

    åŠŸèƒ½ï¼š
    - æ¥æ”¶è‡ªç„¶è¯­è¨€å‘½ä»¤
    - ä½¿ç”¨ LLM è§£æç”¨æˆ·æ„å›¾
    - è°ƒç”¨ Nav2 æ‰§è¡Œå¯¼èˆªä»»åŠ¡
    - è¿”å›æ‰§è¡Œç»“æœ
    """

    def __init__(self):
        super().__init__("llm_nav_agent")

        # ä½¿ç”¨å¯é‡å…¥å›è°ƒç»„ï¼Œå…è®¸å¹¶å‘å¤„ç†
        self.callback_group = ReentrantCallbackGroup()

        # ============ å£°æ˜å‚æ•° ============
        self._declare_parameters()

        # ============ è·å–å‚æ•° ============
        self.use_local_llm = self.get_parameter("use_local_llm").value
        self.openai_model = self.get_parameter("openai_model").value
        self.openai_temperature = self.get_parameter("openai_temperature").value
        self.ollama_model = self.get_parameter("ollama_model").value
        self.ollama_base_url = self.get_parameter("ollama_base_url").value
        self.max_iterations = self.get_parameter("max_iterations").value
        self.verbose = self.get_parameter("verbose").value
        self.max_history_length = self.get_parameter("max_history_length").value
        self.command_topic = self.get_parameter("command_topic").value
        self.response_topic = self.get_parameter("response_topic").value
        self.status_topic = self.get_parameter("status_topic").value

        # ============ åˆå§‹åŒ–ç»„ä»¶ ============
        self.prompt_manager: PromptManager = get_prompt_manager()
        self.location_manager: LocationManager = get_default_location_manager()

        # è®¾ç½®ä½ç½®ç®¡ç†å™¨åˆ°å·¥å…·åŒ…
        Nav2ToolKit.set_location_manager(self.location_manager)

        # ============ åˆå§‹åŒ– LLM ============
        self._init_llm()

        # ============ åˆå§‹åŒ– Agent ============
        self._init_agent()

        # ============ å¯¹è¯å†å² ============
        self.chat_history: List[BaseMessage] = []
        self.history_lock = threading.Lock()

        # ============ ROS2 æ¥å£ ============
        # è®¢é˜…å‘½ä»¤è¯é¢˜
        self.command_sub = self.create_subscription(
            String,
            self.command_topic,
            self.command_callback,
            10,
            callback_group=self.callback_group,
        )

        # å‘å¸ƒå“åº”è¯é¢˜
        self.response_pub = self.create_publisher(String, self.response_topic, 10)

        # å‘å¸ƒçŠ¶æ€è¯é¢˜
        self.status_pub = self.create_publisher(String, self.status_topic, 10)

        # ============ ç­‰å¾… Nav2 å°±ç»ª ============
        self._publish_status("æ­£åœ¨ç­‰å¾… Nav2 å°±ç»ª...")
        self.get_logger().info("â³ ç­‰å¾… Nav2 å¯¼èˆªæ ˆå°±ç»ª...")

        # åœ¨åå°çº¿ç¨‹ä¸­ç­‰å¾… Nav2
        self.nav2_ready = False
        self.nav2_init_thread = threading.Thread(target=self._wait_for_nav2)
        self.nav2_init_thread.start()

        self.get_logger().info("ğŸš€ LLM Nav Agent èŠ‚ç‚¹å·²å¯åŠ¨ï¼")
        self.get_logger().info(f"ğŸ“¡ ç›‘å¬å‘½ä»¤è¯é¢˜: {self.command_topic}")
        self.get_logger().info(f"ğŸ“¡ å‘å¸ƒå“åº”è¯é¢˜: {self.response_topic}")

    def _declare_parameters(self):
        """å£°æ˜æ‰€æœ‰ ROS2 å‚æ•°"""
        # LLM é…ç½®
        self.declare_parameter("use_local_llm", False)
        self.declare_parameter("openai_model", "gpt-4o")
        self.declare_parameter("openai_temperature", 0.0)
        self.declare_parameter("openai_timeout", 30.0)
        self.declare_parameter("ollama_model", "qwen2.5:7b")
        self.declare_parameter("ollama_base_url", "http://localhost:11434")

        # Agent é…ç½®
        self.declare_parameter("max_iterations", 5)
        self.declare_parameter("verbose", True)
        self.declare_parameter("handle_parsing_errors", True)

        # å¯¹è¯é…ç½®
        self.declare_parameter("max_history_length", 20)
        self.declare_parameter("enable_memory", True)

        # å¯¼èˆªé…ç½®
        self.declare_parameter("nav_timeout", 120.0)
        self.declare_parameter("locations_file", "config/locations.yaml")

        # è¯é¢˜é…ç½®
        self.declare_parameter("command_topic", "/llm_agent/command")
        self.declare_parameter("response_topic", "/llm_agent/response")
        self.declare_parameter("status_topic", "/llm_agent/status")

    def _init_llm(self):
        """åˆå§‹åŒ–å¤§è¯­è¨€æ¨¡å‹"""
        if self.use_local_llm:
            self.get_logger().info(f"ğŸ”§ ä½¿ç”¨æœ¬åœ° Ollama æ¨¡å‹: {self.ollama_model}")
            self.llm = ChatOllama(
                model=self.ollama_model,
                base_url=self.ollama_base_url,
                temperature=0,
            )
        else:
            self.get_logger().info(f"ğŸŒ ä½¿ç”¨ OpenAI æ¨¡å‹: {self.openai_model}")
            # æ£€æŸ¥ API Key
            api_key = os.environ.get("OPENAI_API_KEY")
            if not api_key:
                self.get_logger().warn("âš ï¸ æœªè®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡ï¼")

            self.llm = ChatOpenAI(
                model=self.openai_model,
                temperature=self.openai_temperature,
            )

    def _init_agent(self):
        """åˆå§‹åŒ– LangChain Agent"""
        # è·å–ç³»ç»Ÿæç¤ºè¯
        system_prompt = self.prompt_manager.get_system_prompt()

        # åˆ›å»ºæç¤ºæ¨¡æ¿
        prompt = ChatPromptTemplate.from_messages(
            [
                ("system", system_prompt),
                MessagesPlaceholder(variable_name="chat_history"),
                ("human", "{input}"),
                MessagesPlaceholder(variable_name="agent_scratchpad"),
            ]
        )

        # åˆ›å»º Agent
        agent = create_tool_calling_agent(
            llm=self.llm, tools=ALL_NAV_TOOLS, prompt=prompt
        )

        # åˆ›å»º Agent æ‰§è¡Œå™¨
        self.agent_executor = AgentExecutor(
            agent=agent,
            tools=ALL_NAV_TOOLS,
            verbose=self.verbose,
            handle_parsing_errors=True,
            max_iterations=self.max_iterations,
            return_intermediate_steps=False,
        )

        self.get_logger().info("âœ… LangChain Agent åˆå§‹åŒ–å®Œæˆ")

    def _wait_for_nav2(self):
        """ç­‰å¾… Nav2 å°±ç»ªï¼ˆåœ¨åå°çº¿ç¨‹ä¸­è¿è¡Œï¼‰"""
        try:
            success = Nav2ToolKit.wait_for_nav2(timeout=60.0)
            if success:
                self.nav2_ready = True
                self.get_logger().info("âœ… Nav2 å¯¼èˆªæ ˆå·²å°±ç»ªï¼")
                self._publish_status("Nav2 å°±ç»ªï¼Œå¯ä»¥æ¥æ”¶å‘½ä»¤")
            else:
                self.get_logger().error("âŒ Nav2 åˆå§‹åŒ–è¶…æ—¶")
                self._publish_status("Nav2 åˆå§‹åŒ–å¤±è´¥")
        except Exception as e:
            self.get_logger().error(f"âŒ Nav2 åˆå§‹åŒ–é”™è¯¯: {e}")
            self._publish_status(f"Nav2 é”™è¯¯: {e}")

    def _publish_status(self, status: str):
        """å‘å¸ƒçŠ¶æ€æ¶ˆæ¯"""
        msg = String()
        msg.data = status
        self.status_pub.publish(msg)

    def _publish_response(self, response: str):
        """å‘å¸ƒå“åº”æ¶ˆæ¯"""
        msg = String()
        msg.data = response
        self.response_pub.publish(msg)

    def _update_chat_history(self, user_input: str, response: str):
        """æ›´æ–°å¯¹è¯å†å²"""
        with self.history_lock:
            self.chat_history.append(HumanMessage(content=user_input))
            self.chat_history.append(AIMessage(content=response))

            # é™åˆ¶å†å²é•¿åº¦
            if len(self.chat_history) > self.max_history_length:
                self.chat_history = self.chat_history[-self.max_history_length :]

    def _get_chat_history(self) -> List[BaseMessage]:
        """è·å–å¯¹è¯å†å²çš„å‰¯æœ¬"""
        with self.history_lock:
            return self.chat_history.copy()

    def clear_chat_history(self):
        """æ¸…é™¤å¯¹è¯å†å²"""
        with self.history_lock:
            self.chat_history.clear()
        self.get_logger().info("ğŸ—‘ï¸ å¯¹è¯å†å²å·²æ¸…é™¤")

    def command_callback(self, msg: String):
        """
        å¤„ç†ç”¨æˆ·å‘½ä»¤çš„å›è°ƒå‡½æ•°

        Args:
            msg: åŒ…å«ç”¨æˆ·å‘½ä»¤çš„ ROS2 æ¶ˆæ¯
        """
        user_input = msg.data.strip()

        if not user_input:
            return

        self.get_logger().info(f"ğŸ“© æ”¶åˆ°æŒ‡ä»¤: {user_input}")
        self._publish_status("æ­£åœ¨å¤„ç†å‘½ä»¤...")

        # å¤„ç†ç‰¹æ®Šå‘½ä»¤
        if user_input.lower() in ["clear", "reset", "æ¸…é™¤å†å²", "é‡ç½®"]:
            self.clear_chat_history()
            response = "å·²æ¸…é™¤å¯¹è¯å†å²ï¼Œæˆ‘ä»¬å¯ä»¥é‡æ–°å¼€å§‹ã€‚"
            self._publish_response(response)
            return

        if user_input.lower() in ["help", "å¸®åŠ©", "?"]:
            response = self.prompt_manager.get_welcome_message()
            self._publish_response(response)
            return

        # æ£€æŸ¥ Nav2 æ˜¯å¦å°±ç»ª
        if not self.nav2_ready:
            response = "â³ å¯¼èˆªç³»ç»Ÿæ­£åœ¨åˆå§‹åŒ–ä¸­ï¼Œè¯·ç¨å€™..."
            self._publish_response(response)
            return

        try:
            # è°ƒç”¨ Agent å¤„ç†ç”¨æˆ·è¾“å…¥
            result = self.agent_executor.invoke(
                {"input": user_input, "chat_history": self._get_chat_history()}
            )

            response = result.get("output", "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•å¤„ç†è¿™ä¸ªè¯·æ±‚ã€‚")

            # æ›´æ–°å¯¹è¯å†å²
            self._update_chat_history(user_input, response)

            self.get_logger().info(f"ğŸ¤– å›å¤: {response}")
            self._publish_status("å‘½ä»¤å¤„ç†å®Œæˆ")

        except Exception as e:
            error_msg = str(e)
            self.get_logger().error(f"âŒ å¤„ç†å‘½ä»¤æ—¶å‡ºé”™: {error_msg}")
            response = self.prompt_manager.get_error_prompt(error_msg)
            self._publish_status(f"é”™è¯¯: {error_msg}")

        # å‘å¸ƒå“åº”
        self._publish_response(response)

    def destroy_node(self):
        """æ¸…ç†èµ„æº"""
        self.get_logger().info("ğŸ›‘ æ­£åœ¨å…³é—­ LLM Nav Agent...")

        # ç­‰å¾… Nav2 åˆå§‹åŒ–çº¿ç¨‹å®Œæˆ
        if hasattr(self, "nav2_init_thread") and self.nav2_init_thread.is_alive():
            self.nav2_init_thread.join(timeout=2.0)

        # å…³é—­ Nav2
        try:
            Nav2ToolKit.shutdown()
        except Exception as e:
            self.get_logger().warn(f"å…³é—­ Nav2 æ—¶å‡ºé”™: {e}")

        super().destroy_node()


def main(args=None):
    """ä¸»å‡½æ•°"""
    rclpy.init(args=args)

    node = LLMNavAgentNode()

    # ä½¿ç”¨å¤šçº¿ç¨‹æ‰§è¡Œå™¨ä»¥æ”¯æŒå¹¶å‘å›è°ƒ
    executor = MultiThreadedExecutor(num_threads=4)
    executor.add_node(node)

    try:
        executor.spin()
    except KeyboardInterrupt:
        node.get_logger().info("âŒ¨ï¸ æ”¶åˆ°é”®ç›˜ä¸­æ–­ä¿¡å·")
    except Exception as e:
        node.get_logger().error(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")
    finally:
        node.destroy_node()
        rclpy.shutdown()


if __name__ == "__main__":
    main()
